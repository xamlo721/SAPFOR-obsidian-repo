Пример показывает распределение память только с использованием MPI


```fortran
PROGRAM JACOBI_MPI

include 'mpif.h'
integer me, nprocs
PARAMETER(L=4096, ITMAX=100, LC=2, LR=2)
REAL A(0:L/LR+1,0:L/LC+1), B(L/LR,L/LC)
// C arrays A and B wish block distribution
integer dim(2), coords(2)
logical isper(2)
integer status(MPI_STATUS_SIZE, 4), req(8), newcomm
integer strow, lrow, nrow, scol, lcol, ncol
integer pup, pdown, pleft, pright
dim(1) = LR
dim(2) = LC
isper(1) = .false.
isper(2) = .false.
reor = .true.

call MPI_Init(ierr)
call MPI_Comm_rank(mpi_comm_world, me, ierr)
call MPI_Comm_size(mpi_comm_world, nprocs, ierr)
call MPI_Cart_create(mpi_comm_world, 2, dim, isper, * .true., newcomm, ierr)
call MPI_Cart_shift(newcomm, 0, 1, pup, pdown, ierr)
call MPI_Cart_shift(newcomm, 1, 1, pleft, pright, ierr)
call MPI_Comm_rank(newcomm, me, ierr)
call MPI_Cart_coords(newcomm, me, 2, coords, ierr)

// C rows of matrix I have to process

srow = (coords(1) * L) / dim(1)
lrow = (((coords(1) + 1) * L) / dim(1)) -1 
nrow = lrow - srow + 1

// C colomns of matrix I have to process

scol= (coords(2) * L) / dim(2)
lcol = (((coords(2) + 1) * L) / dim(2)) -1 
ncol = lcol- scol + 1

call MPI_Type_vector(ncol, 1, nrow + 2, MPI_DOUBLE_PRECISION, vectype, ierr)
call MPI_Type_commit(vectype, ierr)

IF (me .eq. 0) PRINT *, '********* TEST_JACOBI ********'
	DO IT = 1, ITMAX
		DO J = 1, ncol
			DO I = 1, nrow
				A(I, J) = B(I, J)
			ENDDO
		ENDDO

	// C copyring shadow elements of array A from
	// C neighbouring processors before loop execution

	call MPI_Irecv(A(1,0),nrow,MPI_DOUBLE_PRECISION, pleft, 1235, MPI_COMM_WORLD, req(1), ierr)
	
	call MPI_Isend(A(1,ncol),nrow,MPI_DOUBLE_PRECISION, pright, 1235, MPI|_COMM_WORLD,req(2), ierr)
	call MPI_Irecv(A(1,ncol+1),nrow,MPI_DOUBLE_PRECISION, pright, 1236, MPI_COMM_WORLD, req(3), ierr)
	call MPI_Isend(A(1,1),nrow,MPI_DOUBLE_PRECISION, pleft, 1236, MPI_COMM_WORLD,req(4), ierr)
	
	call MPI_Irecv(A(0,1),1,vectype, pup, 1237, MPI_COMM_WORLD, req(5), ierr)
	call MPI_Isend(A(nrow,1),1,vectype, pdown, 1237, MPI_COMM_WORLD,req(6), ierr)
	call MPI_Irecv(A(nrow+1,1),1,vectype, pdown, 1238, MPI_COMM_WORLD, req(7), ierr)
	call MPI_Isend(A(1,1),1,vectype, pup, 1238, MPI_COMM_WORLD,req(8), ierr)
	
	call MPI_Waitall(8,req,status, ierr)

	DO J =2, ncol-1
		DO! =2, nrow-1
			B(I, J) = (A( 1-1, J) + ACI, J-1.) + A( 144, J) + AC, J#1.)) /4
		ENDDO
	ENDDO
ENDDO

call PRINT_ARRAY_MPI(B)
call MPI_Finalize(ierr)
END
```

